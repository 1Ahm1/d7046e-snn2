{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "colab": {
   "name": "SNN-Excercise-2_2021.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4S1chIkSCU8"
   },
   "source": [
    "# 1 Synaptic plasticity  and learning in SNN – D7046E @ LTU.SE\n",
    "\n",
    "**Last updated 2024-02-01**\n",
    "\n",
    "Both artificial neural networks (ANNs) and spiking neural networks (SNNs) can be optimized with gradient based learning using the back-propagation algorithm, which you are familiar with from the first ANN exercise. [snnTorch](https://snntorch.readthedocs.io/en/latest/readme.html) and [SpyTorch](https://github.com/fzenke/spytorch) are two examples of machine learning libraries that can be used to implement and train time-discretized SNNs with back-prop.\n",
    "\n",
    "However, this is not how brains function. The flexible adaptation and learning capabilities of brains emerge from plasticity mechanisms at the synapse and neuron level that are essentialy *local*, in combination with network architectures featuring sparse long-range connections that have evolved to make the networks adapt in a useful way via plasticity (you can learn more about this in D7064E). This means that the adaptation of neurons and synapses mainly depend on the relative timing and statistics of pre- and postsynaptic spikes. This concept is called *spike timing dependent plasticity* (STDP).\n",
    "\n",
    "In this exercise you will perform simulation experiments with the STDP learning rule for excitatory synapses introduced in the lectures. The particular STDP model that you will investigate is desceribed in [Neuronal Dynamics, 19.2.2 Pair-based Models of STDP](https://neuronaldynamics.epfl.ch/online/Ch19.S2.html). Observe that this model does not apply to inhibitory synapses, which have constant weights in this exercise. You will also use the leaky integrate-and-fire (LIF) model of spiking neurons that you are familiar with from the first SNN exercise.\n",
    "\n",
    "![STDP model](https://neuronaldynamics.epfl.ch/online/x596.png)\n",
    "\n",
    "The basic idea of the pair-based STDP model is illustrated in the figure above. A synapse on a neuron receives a presynaptic spike at time $t^f_i$ and the neuron fires a postsynaptic spike at $t^f_j$. When this happens, the weight of the synapse changes by a relative amount $\\Delta w_{ij}/w_{ij}$ that depends on the relative timing of spikes, $t^f_j-t^f_i$, as illustrated by the experimental results in the figure. The rationale of this is quite:\n",
    "\n",
    "- If the presynaptic spike happens just before the postsynaptic spike there is a cause and effect (causality). The synapse contributes to firing of a spike. In this case the synapse weight is increased (strengthened).\n",
    "- If the presynaptic spike happens just after the postsynaptic spike the latter was fired due to inputs on other synapses and there is *no causal effect*. In this case the synapse weight is decreased (weakened), but not below zero since it remains an excitatory synapse.\n",
    "\n",
    "In general, correlation-based learning of this type is called [Hebbian Learning](https://neuronaldynamics.epfl.ch/online/Ch19.S1.html).\n",
    "\n",
    "The local nature of STDP is key for high energy efficiency. The long-range propagation of derivatives in backpropagation is costly. These aspects have important [carbon emission and environmental consequences](https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem). Neuromorphic computing based on SNNs and STDP [offer an interesting alternative](https://tube.switch.ch/videos/db393d1d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUVLmZJ4SCVB"
   },
   "source": [
    "# 2 Basic SNN Simulator\n",
    "\n",
    "The following basic SNN simulator is familiar from the first SNN exercise. This code has been extended with a neuron type implementing pair-based STDP synapses, as described in [Neuronal Dynamics, 19.2.2 Pair-based Models of STDP](https://neuronaldynamics.epfl.ch/online/Ch19.S2.html).\n",
    "\n",
    "## 2.1 Libraries and generic functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HIqGqT6gSCVC"
   },
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from IPython.display import clear_output\n",
    "#from IPython.core.debugger import set_trace # Activates debugging features\n",
    "\n",
    "def rasterplot(ax, x, y, x_label, y_label):\n",
    "# Function used to plot spike times\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.scatter(x, y, marker='|')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbU7d9QNSCVD"
   },
   "source": [
    "## 2.2 Neuron constructors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "omtATltVSCVE"
   },
   "source": [
    "def lif_neuron(I_inject, E_L=-65e-3, u_reset=-65e-3, u_thres=-50e-3, R=90e6, tau_m=30e-3):\n",
    "    # LIF neuron with a constant injection current\n",
    "    return {\n",
    "        'type'    : 'lif',\n",
    "        'u'       : u_reset,                   # Membrane potential [Volt]\n",
    "        'E_L'     : E_L,                       # Leakage resting potential [Volt]\n",
    "        'u_reset' : u_reset,                   # Reset potential after spike [Volt]\n",
    "        'u_thres' : u_thres,                   # Threshold for spike generation [Volt]\n",
    "        'R'       : R,                         # Membrane resistance [Ohm]\n",
    "        'tau_m'   : tau_m,                     # Membrane time constant [second]\n",
    "        'I_inj'   : I_inject,                  # Injection current [Ampere]\n",
    "    }\n",
    "\n",
    "def lif_syn_neuron(num_synapses, E_L=-65e-3, u_reset=-65e-3, u_thres=-50e-3, R=90e6, tau_m=30e-3, I_inject=0, tau_syn=50e-3):\n",
    "    # LIF neuron with dynamic synapses\n",
    "    n = lif_neuron(I_inject, E_L, u_reset, u_thres, R, tau_m)\n",
    "    n['tau_syn'] = tau_syn                     # Synapse time constant [second] (can also be an array)\n",
    "    n['I_syn']   = np.zeros(num_synapses)      # Postsynaptic currents [Ampere]\n",
    "    n['w_syn']   = np.zeros(num_synapses)      # Synaptic weights [Ampere]\n",
    "    n['type']    = 'lif_syn'\n",
    "    return n\n",
    "\n",
    "def lif_stdp_neuron(num_synapses, E_L=-65e-3, u_reset=-65e-3, u_thres=-50e-3, R=90e6, tau_m=30e-3, I_inject=0,\n",
    "                    tau_syn=50e-3, tau_pls=20e-3, tau_mns=20e-3, w_max=1e-9, w_min=1e-12, gamma=1):\n",
    "    # LIF neuron with dynamic synapses and pair-based STDP\n",
    "    n = lif_syn_neuron(num_synapses, E_L, u_reset, u_thres, R, tau_m, I_inject, tau_syn)\n",
    "    n['x_pre']   = np.zeros(num_synapses)      # STDP trace of presynaptic spikes\n",
    "    n['y_pst']   = 0                           # STDP trace of postsynaptic spikes (scalar, one neuron)\n",
    "    n['tau_pls'] = tau_pls                     # STDP trace time constant [second] (can also be an array)\n",
    "    n['tau_mns'] = tau_mns                     # STDP trace time constant [second] (can also be an array)\n",
    "    n['w_max']   = w_max                       # Largest allowed value of synapse conductance\n",
    "    n['w_min']   = w_min                       # Lowest allowed value of synapse conductance\n",
    "    n['gamma']   = gamma                       # Learning rate parameter with soft bounds (w_min,w_max)\n",
    "    n['type']    = 'lif_stdp'\n",
    "    return n\n",
    "\n",
    "def poisson_neuron(spike_frequency):\n",
    "    # Random spike generator with Poisson distributed spike time intervals, see Section 7.2.1 in the book\n",
    "    return {\n",
    "        'type'      : 'poisson',\n",
    "        'frequency' : spike_frequency          # Average spiking frequency\n",
    "    }\n",
    "\n",
    "def spike_generator(spike_times):\n",
    "    # Generates spikes at time points defined by the sorted list 'spike_t'\n",
    "    return {\n",
    "        'type'      : 'generator',\n",
    "        'spike_t'   : spike_times              # Array of spike times, separated by at least dt\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZNoVAlJSCVE"
   },
   "source": [
    "## 2.3 Network update functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mvbNnBcxSCVF"
   },
   "source": [
    "def integrate(dt, t, neurons):\n",
    "    # Integrate the membrane potential, postsynaptic currents etc one timestep dt\n",
    "    for n in neurons:\n",
    "        if n['type'] == 'lif':\n",
    "            # Integrate membrane potential\n",
    "            dudt = (n['E_L'] - n['u'] + n['R']*n['I_inj']) / n['tau_m']\n",
    "            n['u'] += dt*dudt\n",
    "        elif n['type'] in ['lif_syn','lif_stdp']:\n",
    "            # Integrate array of postsynaptic currents, one current for each synapse\n",
    "            didt = np.divide(-n['I_syn'], n['tau_syn'])\n",
    "            n['I_syn'] += dt*didt\n",
    "            # Integrate membrane potential\n",
    "            dudt = (n['E_L'] - n['u'] + n['R']*(n['I_inj']+sum(n['I_syn']))) / n['tau_m']\n",
    "            n['u'] += dt*dudt\n",
    "            # Integrate local traces for pair-based plasticity\n",
    "            if n['type'] == 'lif_stdp':\n",
    "                n['x_pre'] -= dt*np.divide(n['x_pre'], n['tau_pls'])       # Eq 19.12\n",
    "                n['y_pst'] -= dt*np.divide(n['y_pst'], n['tau_mns'])       # Eq 19.13\n",
    "            \n",
    "def spikegen(dt, t, neurons):\n",
    "    # Implements the non-linear spike generation mechanism\n",
    "    spikes = []\n",
    "    for i,n in enumerate(neurons):\n",
    "        if n['type'] in ['lif','lif_syn','lif_stdp']:\n",
    "            if n['u'] > n['u_thres']:\n",
    "                n['u'] = n['u_reset']\n",
    "                spikes.append(i)\n",
    "        elif n['type'] == 'poisson':\n",
    "            if np.random.rand() < dt*n['frequency']:\n",
    "                spikes.append(i)\n",
    "        elif n['type'] == 'generator':\n",
    "            j = np.searchsorted(n['spike_t'], t, side='right')\n",
    "            if j>0 and t-n['spike_t'][j-1]<dt:\n",
    "                spikes.append(i)\n",
    "                \n",
    "    return spikes\n",
    "\n",
    "def update(dt, t, neurons, connections):\n",
    "    # Update the state of a spiking neural network.\n",
    "    # Refer to Exercise 3 for a reminder about how to set up connections between neurons.\n",
    "    integrate(dt, t, neurons)\n",
    "    spikes = spikegen(dt, t, neurons)\n",
    "\n",
    "    # Update weights and STDP trace for each postsynaptic spike\n",
    "    for spike in spikes:\n",
    "        n = neurons[spike]\n",
    "        if n['type'] in ['lif_stdp']:\n",
    "            n['y_pst'] += 1                                         # Eq 19.13\n",
    "            for i,w in enumerate(n['w_syn']):\n",
    "                if w > 0:                                           # Excitatory synapses\n",
    "                    Aplus = n['gamma']*(n['w_max'] - n['w_syn'][i]) # Eq 19.4\n",
    "                    n['w_syn'][i] += dt*Aplus*n['x_pre'][i]         # Eq 19.14\n",
    "    \n",
    "    # Update synapse currents, weights and STDP traces for each presynaptic spike\n",
    "    for (post, syn, pre) in connections:\n",
    "        for spike in spikes:\n",
    "            if spike == pre:\n",
    "                n = neurons[post]\n",
    "                \n",
    "                if n['type'] not in ['lif_syn', 'lif_stdp']:\n",
    "                    print('Error: Spike sent to neuron type without synapses')\n",
    "                \n",
    "                # Update synapse currents\n",
    "                if n['type'] in ['lif_syn','lif_stdp']:\n",
    "                    n['I_syn'][syn] += n['w_syn'][syn]\n",
    "                    \n",
    "                # Update STDP trace and weight\n",
    "                if n['type'] in ['lif_stdp']:\n",
    "                    n['x_pre'][syn] += 1                                   # Eq 19.12\n",
    "                    if n['w_syn'][syn] > 0:                                # Excitatory synapses\n",
    "                        Aminus = n['gamma']*(n['w_min'] - n['w_syn'][syn]) # Eq 19.4\n",
    "                        n['w_syn'][syn] += dt*Aminus*n['y_pst']            # Eq 19.14\n",
    "    \n",
    "    return spikes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6ULwhuTSCVG"
   },
   "source": [
    "# 3 Tasks\n",
    "\n",
    "## 3.1 Synaptic plasticity experiment\n",
    "\n",
    "**Task 1:** Connect a poisson neuron with 20 Hz spike frequency to one STDP synapse on a LIF neuron. Set the soft bounds on the weight to (w_min=0, w_max=300e-12) and the learning rate to a high value (eg gamma=10) to speed up the weight changes. Set the initial weight of the STDP synapse to 200e-12 and simulate the resulting two-neuron network. Prepare a plot like Figure 19.6 in [Section 19.2](https://neuronaldynamics.epfl.ch/online/Ch19.S2.html) and study the relation between the weight updates and the pre- and postsynaptic spike times. Why does the weight decrease at certain times, and increase at other times?\n",
    "\n",
    "**Answer:** The weight increases when ... and decreases when ...\n",
    "\n",
    "### 3.1.1 Define neurons and connections"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UhwhxXaySCVG"
   },
   "source": [
    "# 20 Hz poisson neuron\n",
    "n0 = poisson_neuron(20)\n",
    "\n",
    "# LIF neuron with one STDP synapse\n",
    "n_stdp = lif_stdp_neuron( ... )\n",
    "n_stdp['w_syn'][0] = ...\n",
    "\n",
    "# Neuron population\n",
    "neurons = [n0, n_stdp]\n",
    "\n",
    "# Connect n0 to n_stdp\n",
    "connections = [...]   # [post,syn,pre], see former SNN exercise\n",
    "\n",
    "# Init time coordinate and stepsize\n",
    "t = 0\n",
    "dt = 1e-4"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXUfMT7PSCVH"
   },
   "source": [
    "### 3.1.2 Simulate the network and plot results\n",
    "\n",
    "The code below can be executed multiple times to advance the simulation one second"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9_ZddZRvSCVH"
   },
   "source": [
    "t0 = t\n",
    "tv = []\n",
    "x = []\n",
    "y = []\n",
    "w = []\n",
    "t_spike = []\n",
    "n_spike = []\n",
    "\n",
    "while t < t0+1:\n",
    "\n",
    "    spikes = update(dt, t, neurons, connections)\n",
    "    \n",
    "    # Store membrane potentials needed for plotting/analysis\n",
    "    tv.append(t)\n",
    "    x.append(list(n_stdp['x_pre']))\n",
    "    y.append(n_stdp['y_pst'])\n",
    "    w.append(list(n_stdp['w_syn']/1e-12))\n",
    "    \n",
    "    # Store spikes needed for plotting/analysis\n",
    "    if len(spikes)>0:\n",
    "        for s in spikes:\n",
    "            t_spike.append(t)\n",
    "            n_spike.append(s)\n",
    "\n",
    "    # Timestep completed\n",
    "    t += dt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(4,1, sharex=True)\n",
    "ax1.plot(tv,x); ax1.set_ylabel('x_pre'); ax1.legend(['syn 0'])\n",
    "ax2.plot(tv,y); ax2.set_ylabel('y_pst')\n",
    "ax3.plot(tv,w); ax3.set_ylabel('w'); ax3.legend(['syn 0'])\n",
    "rasterplot(ax4, t_spike, n_spike,'Time [s]','Neuron index')\n",
    "#fig.savefig('stdp_example.pdf',format='pdf')\n",
    "\n",
    "#print(n_spike.count(0))\n",
    "#print(n_spike.count(1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T5KrqXNSCVI"
   },
   "source": [
    "**Task 2:** What happens if you set the initial weight of the STDP synapse to 150e-12 and simulate the resulting network? Why is the behaviour different compared to the result obtained in Task 1? (Hint: Estimate the number of postsynaptic spikes per second of simulation time and compare that with the presynaptic spikerate)\n",
    "\n",
    "**Answer:** The ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HYYEnHuISCVI"
   },
   "source": [
    "\n",
    "# Code for Task 2\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LADnQMrSCVI"
   },
   "source": [
    "**Task 3:** What happens if you set the initial weight of the STDP synapse to 300e-12 and simulate the resulting network? Why is the behaviour different compared to the result obtained in Task 1 and Task 2?\n",
    "\n",
    "**Answer:** When ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8Qqge5mbSCVJ"
   },
   "source": [
    "\n",
    "# Code for Task 3\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkG29sjBSCVJ"
   },
   "source": [
    "**Task 4:** Describe the results in Tasks 1-3 in terms of Hebbian Learning.\n",
    "\n",
    "*Hint: The weight influences the spikerate of the postsynaptic neuron. Consider the effect of different spikerates of the postsynaptic neuron on the relative timing of pre- and postsynaptic spikes (on the average).*\n",
    "\n",
    "**Answer:** Hebbian learning ... In Task # ... when ...\n",
    "\n",
    "\n",
    "**Task 5:** Describe the relation between this implementation of STDP and the concept illustrated in [Figure 19.4F](https://neuronaldynamics.epfl.ch/online/Ch19.S1.html) in Neuronal Dynamics. You have to study the pair-based STDP equations and related figures/text in the book to understand the relation.\n",
    "\n",
    "**Answer:** This implementation is based on equations ..., which ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COw87zHbSCVJ"
   },
   "source": [
    "## 3.2 Learning of MNIST digits and classification\n",
    "\n",
    "SNNs with STDP can adapt to firing patterns in the stimulus. The tasks above illustrates that at the single synapse and neuron level.\n",
    "\n",
    "With multiple cells, and competition between the excitatory cells via mutual/lateral inhibition, the different excitatory cells adapt to different stimuli. Such networks can perform clustering of stimuli via activation of different cell assemblies, and forms a basis for [decision making networks](https://neuronaldynamics.epfl.ch/online/Ch16.html).\n",
    "\n",
    "A network of this kind can for example be used to classify inputs, or to learn [visual receptive fields](https://doi.org/10.1523/JNEUROSCI.4188-12.2013). In this task you will implement some parts of the MNIST classification network presented in the paper [unsupervised learning of digit recognition using spike-timing-dependent plasticity](https://doi.org/10.3389/fncom.2015.00099) and study the STDP of synapses on two excitatory cells when digits from the MNIST dataset are presented to the network.\n",
    "\n",
    "![Network](https://www.frontiersin.org/files/Articles/149773/fncom-09-00099-r4/image_m/fncom-09-00099-g001.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzPFyyWkSCVK"
   },
   "source": [
    "### 3.2.1 Load MNIST samples\n",
    "\n",
    "You will only consider two digits and two excitatory cells to reduce the simulation time and complexity of the task (see the [paper](https://doi.org/10.3389/fncom.2015.00099) for complete results)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajD3quOgSCVK"
   },
   "source": [
    "def plot_digit(plt, data):\n",
    "# Display MNIST images\n",
    "    data = data.view(28,28)\n",
    "    plt.imshow(data, cmap='gray')\n",
    "\n",
    "mnist_train = datasets.MNIST('./', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist_train, batch_size=1000, shuffle=False)\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Extract 10 training samples for two classes of digits\n",
    "im3 = images[labels==3][0:10]\n",
    "im5 = images[labels==5][0:10]\n",
    "print('%d training inputs loaded' %(len(im3)+len(im5)))\n",
    "del images, labels, mnist_train, train_loader\n",
    "\n",
    "# Normalization of inputs\n",
    "nf = (im3.sum() + im5.sum())/(len(im3)+len(im5))\n",
    "for i in range(10):\n",
    "    im3[i] = im3[i]*nf/im3[i].sum()\n",
    "    im5[i] = im5[i]*nf/im5[i].sum()\n",
    "\n",
    "# Plot stimuli intensities\n",
    "plt.rcParams['figure.figsize'] = [16, 4]\n",
    "fig, axs = plt.subplots(2, 10,sharey=True)\n",
    "for i in range(10):\n",
    "    plot_digit(axs[0, i], im3[i])\n",
    "    plot_digit(axs[1, i], im5[i])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qCwH6jFSCVK"
   },
   "source": [
    "### 3.2.2 Define neuron populations and connections\n",
    "\n",
    "There are three neuron populations in the achitecture described in the paper, as illustrated above:\n",
    "\n",
    "- Input: 28x28 randomly spiking neurons with pixel-intensity dependent spike frequencies.\n",
    "- Excitatory neurons: A number of excitatory LIF neurons. 2 in this exercise.\n",
    "- Inhibitory neurons: Here we will **simplify** this part and use two inhibitory synapses instead.\n",
    "\n",
    "Each excitatory neuron recives input from all 28x28 input neurons (all-to-all connectivity) via plastic STDP synapses. The STDP synapse weights are initialized randomly. The excitatory neurons inhibit eachother via one inhibitory synapse on each neuron, thus the name *lateral inhibition*. This simplification is possible in SNNs (but not in biology, since excitatory neurons only form excitatory synapses on the target dendrites)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KkUnuz58SCVK"
   },
   "source": [
    "N_pixls = 28\n",
    "N_input = N_pixls*N_pixls\n",
    "\n",
    "neurons = []\n",
    "connections = []\n",
    "\n",
    "# Create input neurons, frequencies will be set later\n",
    "for i in range(N_input):\n",
    "    neurons.append(poisson_neuron(0))\n",
    "    \n",
    "# Create two excitatory neurons\n",
    "e1 = lif_stdp_neuron(N_input+1, w_min=0, w_max=10e-12)\n",
    "e2 = lif_stdp_neuron(N_input+1, w_min=0, w_max=10e-12)\n",
    "neurons.append(e1)\n",
    "neurons.append(e2)\n",
    "I_e1 = N_input\n",
    "I_e2 = N_input+1\n",
    "\n",
    "# Define excitatory STDP synapses for the inputs\n",
    "for j in range(N_input):\n",
    "    e1['w_syn'][j] = 2e-12*(0.1 + 0.9*np.random.rand())\n",
    "    e2['w_syn'][j] = 2e-12*(0.1 + 0.9*np.random.rand())\n",
    "    connections.append([I_e1,j,j]) # [post,syn,pre], see Exercise 3 for details\n",
    "    connections.append([I_e2,j,j]) # [post,syn,pre]\n",
    "\n",
    "# Lateral inhibition (neglecting Dale's law to simplify the exercise)\n",
    "I_inhib = N_input\n",
    "e1['w_syn'][I_inhib] = -1e-12\n",
    "e2['w_syn'][I_inhib] = -1e-12\n",
    "connections.append([I_e1,I_inhib,I_e2])\n",
    "connections.append([I_e2,I_inhib,I_e1])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_7WB0uIgSCVL"
   },
   "source": [
    "def reset_input_frequencies(neurons):\n",
    "# Reset frequencies of input neurons to 0 Hz\n",
    "    for i in range(N_pixls):\n",
    "        for j in range(N_pixls):\n",
    "            neurons[i*N_pixls+j]['frequency'] = 0\n",
    "\n",
    "def set_input_frequencies(neurons, image):\n",
    "# Convert pixel intensities to spikerates of random (Poisson) neurons\n",
    "    for i in range(N_pixls):\n",
    "        for j in range(N_pixls):\n",
    "            neurons[i*N_pixls+j]['frequency'] = 5 + 55*image[i][j] # 5-60 Hz\n",
    "            \n",
    "def spikerate_filter(spikerate, spikecount, simtime, alpha):\n",
    "# Low-pass filter for estimation of spikerate\n",
    "    if spikerate > 0:\n",
    "        spikerate = alpha*spikerate + (1-alpha)*spikecount/simtime\n",
    "    else:\n",
    "        spikerate = spikecount/simtime\n",
    "    return spikerate\n",
    "\n",
    "def plot_neuron_weights(n):\n",
    "# Plot the input weights of an excitatory neuron (with STDP synapses)\n",
    "    plt.rcParams['figure.figsize'] = [10, 3]\n",
    "    fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "    img = np.zeros((N_pixls,N_pixls))\n",
    "    for i in range(N_pixls):\n",
    "        for j in range(N_pixls):\n",
    "            img[i][j] = n['w_syn'][i*N_pixls+j]/1e-12\n",
    "    ax1.imshow(img, cmap='viridis')\n",
    "    ax2.hist(np.hstack(img))\n",
    "    ax2.set_xlabel('Weight [pA]')\n",
    "    ax2.set_ylabel('No. synapses')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nsEwEUWCSCVL"
   },
   "source": [
    "# Reset the simulation time\n",
    "t = 0\n",
    "dt = 5e-4\n",
    "simtime = 0.35\n",
    "rsttime = 0.15\n",
    "\n",
    "# Reset spikerate variables\n",
    "e1_spikerate = -1\n",
    "e2_spikerate = -1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHJ_L28qSCVL"
   },
   "source": [
    "### 3.2.3 Supervised training of the network\n",
    "\n",
    "Lateral inhibition plays a key role for the learning (clustering) capabilities of the network. Here you will manually control the inhibition in a supervised fashion, and investigate the resulting weights of the excitatory cells.\n",
    "\n",
    "**Task 6:** Modify the weights of the inhibitory synapses on the two excitatory neurons e1 and e2 at the indicated positions in the code below, so that e1 is inhibited when a training sample from one of the two classes of digits is presented, and vice versa. Run 50 training iterations and compare your result with the result already provided below. What values of the inhibitory weights enables the two neurons to adapt to the two classes of stimuli? Describe what happened for different values of the inhibitory weights.\n",
    "\n",
    "**Answer:** With a value of ...\n",
    "\n",
    "**Task 7:** Explain the result obtained in Task 6 in terms of the experience gained from Tasks 1-5. Why does STDP produce this result, how can the neuron weights successfully adapt to the stimuli?\n",
    "\n",
    "**Answer:** In ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OTDlcrb-SCVL"
   },
   "source": [
    "from __future__ import print_function   # Python 2.X support for print function, not needed in Python 3.X\n",
    "\n",
    "# Speed up learning a bit\n",
    "e1['gamma'] = 5\n",
    "e2['gamma'] = 5\n",
    "\n",
    "for iterations in range(50): # Training iterations\n",
    "\n",
    "    # Start time\n",
    "    t0 = t\n",
    "    \n",
    "    # Spike counts of e1 and e2\n",
    "    e1_count = 0\n",
    "    e2_count = 0\n",
    "    \n",
    "    # Sample a different digit every second iteration\n",
    "    stimuli = []\n",
    "    if iterations % 2 == 0:\n",
    "        stimuli = im3[int(10*np.random.rand())][0]\n",
    "        ##############################################################################\n",
    "        # INSERT SUPERVISION SIGNAL HERE, INHIBIT ONE OF THE TWO STDP NEURONS\n",
    "        e1['w_syn'][I_inhib] = ...\n",
    "        e2['w_syn'][I_inhib] = ...\n",
    "        ##############################################################################\n",
    "    else:\n",
    "        stimuli = im5[int(10*np.random.rand())][0]\n",
    "        ##############################################################################\n",
    "        # INSERT SUPERVISION SIGNAL HERE, INHIBIT ONE OF THE TWO STDP NEURONS\n",
    "        e1['w_syn'][I_inhib] = ...\n",
    "        e2['w_syn'][I_inhib] = ...\n",
    "        ##############################################################################\n",
    "    set_input_frequencies(neurons, stimuli)\n",
    "    \n",
    "    # Simulate the network\n",
    "    while t < t0+simtime:\n",
    "    \n",
    "        # Update network, including STDP\n",
    "        spikes = update(dt, t, neurons, connections)\n",
    "        \n",
    "        # Count number of spikes from e1 and e2\n",
    "        e1_count += spikes.count(I_e1)\n",
    "        e2_count += spikes.count(I_e2)\n",
    "        \n",
    "        # Timestep completed\n",
    "        t += dt\n",
    "        print('\\r t = %fs (%d%%)' % (t, 100*(t-t0)/(simtime+rsttime)), end='')\n",
    "\n",
    "    # Let neurons and synapses rest before the next stimuli \n",
    "    reset_input_frequencies(neurons)\n",
    "    while t < t0+simtime+rsttime:\n",
    "        spikes = update(dt, t, neurons, connections)\n",
    "        e1_count += spikes.count(I_e1)\n",
    "        e2_count += spikes.count(I_e2)\n",
    "        t += dt\n",
    "        print('\\r t = %fs (%d%%)' % (t, 100*(t-t0)/(simtime+rsttime)), end='')\n",
    "\n",
    "    # Refresh the plots\n",
    "    clear_output()\n",
    "    print('\\nStimuli:')\n",
    "    plot_digit(plt,stimuli)\n",
    "    plt.show()\n",
    "    print('Excitatory cell e1 spike count: %d, and weight distribution after %d seconds:' % (e1_count,t))\n",
    "    plot_neuron_weights(e1)\n",
    "    print('Excitatory cell e2 spike count: %d, and weight distribution after %d seconds:' % (e2_count,t))\n",
    "    plot_neuron_weights(e2)\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAEUsIiLSCVM"
   },
   "source": [
    "**Task 8:** Next, switch off learning and adjust the lateral inhibition so that your network can classify samples in your training set. How is the predicted class determined? Investigate if you can get 100% correct classification on the training set. (You do not have to consider a validation and test set in this exercise)\n",
    "\n",
    "**Answer:** The predicted class is determined by ... and X out of the Y samples are correctly classified with inhibitory weights ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dqHUj9KPSCVM"
   },
   "source": [
    "e1['gamma'] = 0\n",
    "e2['gamma'] = 0\n",
    "\n",
    "e1['w_syn'][I_inhib] = ...\n",
    "e2['w_syn'][I_inhib] = ...\n",
    "\n",
    "for iterations in range(20):\n",
    "\n",
    "..."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vcGkIuvSCVM"
   },
   "source": [
    "### 3.2.4 Optional task: Unsupervised learning\n",
    "\n",
    "In principle, it is possible to adapt the inhibition of e1 and e2 during training, so that the synapses self-organize via STDP in a similar way as in Section 3.2.3 but without any supervision signal. This requires homeostatic regulation of the e1 and e2 spikerates (described in the lectures and the papers referenced above). The trick here is to establish the right balance between lateral inhibition and spikerate regulation, so that the excitatory cells are mutually activated and deactivated when different stimuli are presented.\n",
    "\n",
    "Can you solve this *challenge*? Sample code is provided below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-kg7sSZiSCVM"
   },
   "source": [
    "simtime = 0.35  # seconds, period when stimuli is presented\n",
    "rsttime = 0.15  # seconds, period when no stimuli is presented\n",
    "\n",
    "for iterations in range(2):\n",
    "\n",
    "    t0 = t\n",
    "\n",
    "    e1_count = 0\n",
    "    e2_count = 0\n",
    "    \n",
    "    # Draw a different digit every second iteration\n",
    "    stimuli = []\n",
    "    if iterations % 2 == 0:\n",
    "        stimuli = im5[int(10*np.random.rand())][0]\n",
    "    else:\n",
    "        stimuli = im3[int(10*np.random.rand())][0]\n",
    "    set_input_frequencies(neurons, stimuli)\n",
    "    \n",
    "    # Stimulate the network\n",
    "    while t < t0+simtime:\n",
    "    \n",
    "        # Update network, including STDP\n",
    "        spikes = update(dt, t, neurons, connections)\n",
    "        \n",
    "        # Count number of spikes from e1 and e2\n",
    "        e1_count += spikes.count(N_input+0)\n",
    "        e2_count += spikes.count(N_input+1)\n",
    "        \n",
    "        # Timestep completed\n",
    "        t += dt\n",
    "        print('\\r t = %fs (%d%%)' % (t, 100*(t-t0)/(simtime+rsttime)), end = '')\n",
    "\n",
    "    # Homeostatic regulation of spikerate, update the spiking threshold of the excitatory neurons\n",
    "    e1_spikerate = spikerate_filter(e1_spikerate, e1_count, simtime, 0.5)\n",
    "    e2_spikerate = spikerate_filter(e2_spikerate, e2_count, simtime, 0.5)\n",
    "    if e1_spikerate   ...\n",
    "        e1['u_thres'] ...\n",
    "    elif e2_spikerate ...\n",
    "        e2['u_thres'] ...\n",
    "        \n",
    "    # Let neurons and synapses rest before the next stimuli \n",
    "    reset_input_frequencies(neurons)\n",
    "    while t < t0+simtime+rsttime:\n",
    "        spikes = update(dt, t, neurons, connections)\n",
    "        e1_count += spikes.count(N_input+0)\n",
    "        e2_count += spikes.count(N_input+1)\n",
    "        t += dt\n",
    "        print('\\r t = %fs (%d%%)' % (t, 100*(t-t0)/(simtime+rsttime)), end = '')\n",
    "\n",
    "    # Refresh the plots\n",
    "    clear_output()\n",
    "    print('\\nStimuli:')\n",
    "    plot_digit(plt,stimuli)\n",
    "    plt.show()\n",
    "    print('Excitatory cell e1 spike count: %d, and weight distribution:' % (e1_count))\n",
    "    plot_neuron_weights(e1)\n",
    "    print('Excitatory cell e2 spike count: %d, and weight distribution:' % (e2_count))\n",
    "    plot_neuron_weights(e2)\n",
    "    print('Spikerates of e1 and e2 are %1.1f Hz and %1.1f Hz, respectively.' % (e1_spikerate,e2_spikerate))\n",
    "    print('Thresholds are %1.4f mV and %1.4f mV, respectively.' % (e1['u_thres'],e2['u_thres']))\n",
    "    print('Inh weights are %1.2e pA and %1.2e pA, respectively.' % (e1['w_syn'][I_inhib],e2['w_syn'][I_inhib]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0IOEBCESCVM"
   },
   "source": [
    "# 4 Comment and further reading\n",
    "\n",
    "In this exercise you have considered a tiny network of two neurons. In the paper considered as inspiration for this exercise the accuracy on the test set increases with the number of excitatory neurons in the population:\n",
    "\n",
    "![MNIST test result](https://www.frontiersin.org/files/Articles/149773/fncom-09-00099-r4/image_m/fncom-09-00099-g002.jpg)\n",
    "\n",
    "An interesting aspect is that the STDP learning rule is a local rule that does not depend on global gradients. The training protocol is also unsupervised up to the point where the activities of different neuron populations are linked to class labels. Tnus, this SNN solves an unsupervised clustering task. After clustering the inputs, a class label can be associated with neurons that are activated for one specific digit. The activity of the subset/population of neurons that tend to fire for one particular input class determine which class the input belongs to.\n",
    "\n",
    "Similar accuracies can be obtained with SNN and ANN; [Spiking neural networks for handwritten digit recognition—Supervised learning and network optimization](https://doi.org/10.1016/j.neunet.2018.03.019)\n",
    "\n",
    "There is more to learn about how different spike codes and learning architectures can be further developed to improve on these results, see for example: [Deep Learning With Spiking Neurons: Opportunities and Challenges](https://doi.org/10.3389/fnins.2018.00774)"
   ]
  }
 ]
}
